{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "modelling.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VsREPJdlJCw",
        "outputId": "4469c91a-6a9c-44ea-fdf3-c3fc6152c9bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting jiwer\n",
            "  Downloading jiwer-2.3.0-py3-none-any.whl (15 kB)\n",
            "Collecting python-Levenshtein==0.12.2\n",
            "  Downloading python-Levenshtein-0.12.2.tar.gz (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 3.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from python-Levenshtein==0.12.2->jiwer) (57.4.0)\n",
            "Building wheels for collected packages: python-Levenshtein\n",
            "  Building wheel for python-Levenshtein (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-Levenshtein: filename=python_Levenshtein-0.12.2-cp37-cp37m-linux_x86_64.whl size=149860 sha256=45cf4cc849b2e1805c44a439bb4b6e9f6a599c2b2f686fed8a5c1261d03a74cb\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/5f/ca/7c4367734892581bb5ff896f15027a932c551080b2abd3e00d\n",
            "Successfully built python-Levenshtein\n",
            "Installing collected packages: python-Levenshtein, jiwer\n",
            "Successfully installed jiwer-2.3.0 python-Levenshtein-0.12.2\n"
          ]
        }
      ],
      "source": [
        "!pip install jiwer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "from jiwer import wer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import FunctionTransformer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JONPcY77lSCT",
        "outputId": "16570db3-785f-4d6f-86df-7e67bab2ecf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"drive/MyDrive/AMHARIC\")\n",
        "os.listdir()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72zyhw88lTWg",
        "outputId": "d7e61989-8b00-4ed9-8654-bb0cc584c558"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['README.md',\n",
              " 'data',\n",
              " 'kaldi-script',\n",
              " 'lang',\n",
              " 'lm',\n",
              " 'metadata.csv',\n",
              " 'realmeta.csv',\n",
              " '.git']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "meta_data = pd.read_csv(\"realmeta.csv\")\n",
        "meta_data. drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
        "meta_data = meta_data.sample(n=100).reset_index().drop(\"index\", axis=1)\n",
        "meta_data['Transcript'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "1MAF05gplUBw",
        "outputId": "659248cf-c7e9-4f46-e8b1-b9844050a457"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'በ ሼህ አላሙዲ ን በጐ ፈቃደኝነት የሚሰሩ ትን ቤቶች ስራ የሚ ቆጣጠሩ የ ኮሚቴ አባላት መንደሩ የ ኩባንያው ን ስ ም እንዲ ወስድ በ ጋራ መወሰናቸው ታውቋል'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "split = int(len(meta_data) * 0.90)\n",
        "df_train = meta_data[:split]\n",
        "df_val = meta_data[split:]\n",
        "\n",
        "print(f\"Size of the training set: {len(df_train)}\")\n",
        "print(f\"Size of the training set: {len(df_val)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LrIUXFbClUeo",
        "outputId": "59516da2-e91e-4299-d211-c8771730a8a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of the training set: 90\n",
            "Size of the training set: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "supported = \"\"\"\n",
        "ሀ ሁ ሂ ሄ ህ ሆ\n",
        "ለ ሉ ሊ ላ ሌ ል ሎ ሏ\n",
        "መ ሙ ሚ ማ ሜ ም ሞ ሟ\n",
        "ረ ሩ ሪ ራ ሬ ር ሮ ሯ\n",
        "ሰ ሱ ሲ ሳ ሴ ስ ሶ ሷ\n",
        "ሸ ሹ ሺ ሻ ሼ ሽ ሾ ሿ\n",
        "ቀ ቁ ቂ ቃ ቄ ቅ ቆ ቋ\n",
        "በ ቡ ቢ ባ ቤ ብ ቦ ቧ\n",
        "ቨ ቩ ቪ ቫ ቬ ቭ ቮ ቯ\n",
        "ተ ቱ ቲ ታ ቴ ት ቶ ቷ\n",
        "ቸ ቹ ቺ ቻ ቼ ች ቾ ቿ\n",
        "ኋ\n",
        "ነ ኑ ኒ ና ኔ ን ኖ ኗ\n",
        "ኘ ኙ ኚ ኛ ኜ ኝ ኞ ኟ\n",
        "አ ኡ ኢ ኤ እ ኦ\n",
        "ኧ\n",
        "ከ ኩ ኪ ካ ኬ ክ ኮ\n",
        "ኳ\n",
        "ወ ዉ ዊ ዋ ዌ ው ዎ\n",
        "ዘ ዙ ዚ ዛ ዜ ዝ ዞ ዟ\n",
        "ዠ ዡ ዢ ዣ ዤ ዥ ዦ ዧ\n",
        "የ ዩ ዪ ያ ዬ ይ ዮ\n",
        "ደ ዱ ዲ ዳ ዴ ድ ዶ ዷ\n",
        "ጀ ጁ ጂ ጃ ጄ ጅ ጆ ጇ\n",
        "ገ ጉ ጊ ጋ ጌ ግ ጐ ጓ ጔ\n",
        "ጠ ጡ ጢ ጣ ጤ ጥ ጦ ጧ\n",
        "ጨ ጩ ጪ ጫ ጬ ጭ ጮ ጯ\n",
        "ጰ ጱ ጲ ጳ ጴ ጵ ጶ ጷ\n",
        "ፀ ፁ ፂ ፃ ፄ ፅ ፆ ፇ\n",
        "ፈ ፉ ፊ ፋ ፌ ፍ ፎ ፏ\n",
        "ፐ ፑ ፒ ፓ ፔ ፕ ፖ\n",
        "\"\"\".split()\n",
        "\n",
        "char_map = {}\n",
        "char_map[\"\"] = 0\n",
        "char_map[\" \"] = 1\n",
        "index = 2\n",
        "for c in supported:\n",
        "    char_map[c] = index\n",
        "    index += 1\n",
        "index_map = {v+1: k for k, v in char_map.items()}\n",
        "lan_lis = list(index_map.values())\n",
        "print(lan_lis)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aaSJKq1wqCNf",
        "outputId": "404843bf-a6da-40ea-9c3e-dfee415fc495"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['', ' ', 'ሀ', 'ሁ', 'ሂ', 'ሄ', 'ህ', 'ሆ', 'ለ', 'ሉ', 'ሊ', 'ላ', 'ሌ', 'ል', 'ሎ', 'ሏ', 'መ', 'ሙ', 'ሚ', 'ማ', 'ሜ', 'ም', 'ሞ', 'ሟ', 'ረ', 'ሩ', 'ሪ', 'ራ', 'ሬ', 'ር', 'ሮ', 'ሯ', 'ሰ', 'ሱ', 'ሲ', 'ሳ', 'ሴ', 'ስ', 'ሶ', 'ሷ', 'ሸ', 'ሹ', 'ሺ', 'ሻ', 'ሼ', 'ሽ', 'ሾ', 'ሿ', 'ቀ', 'ቁ', 'ቂ', 'ቃ', 'ቄ', 'ቅ', 'ቆ', 'ቋ', 'በ', 'ቡ', 'ቢ', 'ባ', 'ቤ', 'ብ', 'ቦ', 'ቧ', 'ቨ', 'ቩ', 'ቪ', 'ቫ', 'ቬ', 'ቭ', 'ቮ', 'ቯ', 'ተ', 'ቱ', 'ቲ', 'ታ', 'ቴ', 'ት', 'ቶ', 'ቷ', 'ቸ', 'ቹ', 'ቺ', 'ቻ', 'ቼ', 'ች', 'ቾ', 'ቿ', 'ኋ', 'ነ', 'ኑ', 'ኒ', 'ና', 'ኔ', 'ን', 'ኖ', 'ኗ', 'ኘ', 'ኙ', 'ኚ', 'ኛ', 'ኜ', 'ኝ', 'ኞ', 'ኟ', 'አ', 'ኡ', 'ኢ', 'ኤ', 'እ', 'ኦ', 'ኧ', 'ከ', 'ኩ', 'ኪ', 'ካ', 'ኬ', 'ክ', 'ኮ', 'ኳ', 'ወ', 'ዉ', 'ዊ', 'ዋ', 'ዌ', 'ው', 'ዎ', 'ዘ', 'ዙ', 'ዚ', 'ዛ', 'ዜ', 'ዝ', 'ዞ', 'ዟ', 'ዠ', 'ዡ', 'ዢ', 'ዣ', 'ዤ', 'ዥ', 'ዦ', 'ዧ', 'የ', 'ዩ', 'ዪ', 'ያ', 'ዬ', 'ይ', 'ዮ', 'ደ', 'ዱ', 'ዲ', 'ዳ', 'ዴ', 'ድ', 'ዶ', 'ዷ', 'ጀ', 'ጁ', 'ጂ', 'ጃ', 'ጄ', 'ጅ', 'ጆ', 'ጇ', 'ገ', 'ጉ', 'ጊ', 'ጋ', 'ጌ', 'ግ', 'ጐ', 'ጓ', 'ጔ', 'ጠ', 'ጡ', 'ጢ', 'ጣ', 'ጤ', 'ጥ', 'ጦ', 'ጧ', 'ጨ', 'ጩ', 'ጪ', 'ጫ', 'ጬ', 'ጭ', 'ጮ', 'ጯ', 'ጰ', 'ጱ', 'ጲ', 'ጳ', 'ጴ', 'ጵ', 'ጶ', 'ጷ', 'ፀ', 'ፁ', 'ፂ', 'ፃ', 'ፄ', 'ፅ', 'ፆ', 'ፇ', 'ፈ', 'ፉ', 'ፊ', 'ፋ', 'ፌ', 'ፍ', 'ፎ', 'ፏ', 'ፐ', 'ፑ', 'ፒ', 'ፓ', 'ፔ', 'ፕ', 'ፖ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wavs_path_train = (\"data/train/wav/\")\n",
        "wavs_path_test = (\"data/test/wav/\")"
      ],
      "metadata": {
        "id": "qlESw6l8rjLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mapping characters to integers\n",
        "char_to_num = keras.layers.StringLookup(vocabulary=lan_lis, oov_token=\"\")\n",
        "# Mapping integers back to original characters\n",
        "num_to_char = keras.layers.StringLookup(\n",
        "    vocabulary=char_to_num.get_vocabulary(), oov_token=\"\", invert=True\n",
        ")"
      ],
      "metadata": {
        "id": "ogzcIuDtlUub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# An integer scalar Tensor. The window length in samples.\n",
        "frame_length = 256\n",
        "# An integer scalar Tensor. The number of samples to step.\n",
        "frame_step = 160\n",
        "# An integer scalar Tensor. The size of the FFT to apply.\n",
        "# If not provided, uses the smallest power of 2 enclosing frame_length.\n",
        "fft_length = 384\n",
        "\n",
        "\n",
        "def encode_single_sample(wav_file, label):\n",
        "    ###########################################\n",
        "    ##  Process the Audio\n",
        "    ##########################################\n",
        "    # 1. Read wav file\n",
        "    #print(tf.get_static_value(wav_file))\n",
        "    file = tf.io.read_file(wavs_path_train + wav_file)\n",
        "    # 2. Decode the wav file\n",
        "    audio, _ = tf.audio.decode_wav(file)\n",
        "    audio = tf.squeeze(audio, axis=-1)\n",
        "    # 3. Change type to float\n",
        "    audio = tf.cast(audio, tf.float32)\n",
        "    # 4. Get the spectrogram\n",
        "    spectrogram = tf.signal.stft(\n",
        "        audio, frame_length=frame_length, frame_step=frame_step, fft_length=fft_length\n",
        "    )\n",
        "    # 5. We only need the magnitude, which can be derived by applying tf.abs\n",
        "    spectrogram = tf.abs(spectrogram)\n",
        "    spectrogram = tf.math.pow(spectrogram, 0.5)\n",
        "    # 6. normalisation\n",
        "    means = tf.math.reduce_mean(spectrogram, 1, keepdims=True)\n",
        "    stddevs = tf.math.reduce_std(spectrogram, 1, keepdims=True)\n",
        "    spectrogram = (spectrogram - means) / (stddevs + 1e-10)\n",
        "    ###########################################\n",
        "    ##  Process the label\n",
        "    ##########################################\n",
        "    # 7. Split the label\n",
        "    label = tf.strings.unicode_split(label, input_encoding=\"UTF-8\")\n",
        "    # 8. Map the characters in label to numbers\n",
        "    label = char_to_num(label)\n",
        "    # 9. Return a dict as our model is expecting two inputs\n",
        "    return spectrogram, label"
      ],
      "metadata": {
        "id": "Nih9uRsnlVZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 3\n",
        "# Define the trainig dataset\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (list(df_train[\"audio\"]), list(df_train[\"Transcript\"]))\n",
        ")\n",
        "train_dataset = (\n",
        "    train_dataset.map(encode_single_sample, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    .padded_batch(batch_size)\n",
        "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        ")\n",
        "# Define the validation dataset\n",
        "validation_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (list(df_val[\"audio\"]), list(df_val[\"Transcript\"]))\n",
        ")\n",
        "validation_dataset = (\n",
        "    validation_dataset.map(encode_single_sample, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    .padded_batch(batch_size)\n",
        "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        ")"
      ],
      "metadata": {
        "id": "SDudniwkqnmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def CTCLoss(y_true, y_pred):\n",
        "    # Compute the training-time loss value\n",
        "    batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n",
        "    input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n",
        "    label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n",
        "\n",
        "    input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
        "    label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
        "\n",
        "    loss = keras.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "1jPV4KQ2qpeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "V397rTIncN7T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(input_dim, output_dim, rnn_layers=5, rnn_units=128):\n",
        "    \"\"\"Model similar to DeepSpeech2.\"\"\"\n",
        "    # Model's input\n",
        "    input_spectrogram = layers.Input((None, input_dim), name=\"input\")\n",
        "    # Expand the dimension to use 2D CNN.\n",
        "    x = layers.Reshape((-1, input_dim, 1), name=\"expand_dim\")(input_spectrogram)\n",
        "    # Convolution layer 1\n",
        "    x = layers.Conv2D(\n",
        "        filters=32,\n",
        "        kernel_size=[11, 41],\n",
        "        strides=[2, 2],\n",
        "        padding=\"same\",\n",
        "        use_bias=False,\n",
        "        name=\"conv_1\",\n",
        "    )(x)\n",
        "    x = layers.BatchNormalization(name=\"conv_1_bn\")(x)\n",
        "    x = layers.ReLU(name=\"conv_1_relu\")(x)\n",
        "    # Convolution layer 2\n",
        "    x = layers.Conv2D(\n",
        "        filters=32,\n",
        "        kernel_size=[11, 21],\n",
        "        strides=[1, 2],\n",
        "        padding=\"same\",\n",
        "        use_bias=False,\n",
        "        name=\"conv_2\",\n",
        "    )(x)\n",
        "    x = layers.BatchNormalization(name=\"conv_2_bn\")(x)\n",
        "    x = layers.ReLU(name=\"conv_2_relu\")(x)\n",
        "    # Reshape the resulted volume to feed the RNNs layers\n",
        "    x = layers.Reshape((-1, x.shape[-2] * x.shape[-1]))(x)\n",
        "    # RNN layers\n",
        "    for i in range(1, rnn_layers + 1):\n",
        "        recurrent = layers.GRU(\n",
        "            units=rnn_units,\n",
        "            activation=\"tanh\",\n",
        "            recurrent_activation=\"sigmoid\",\n",
        "            use_bias=True,\n",
        "            return_sequences=True,\n",
        "            reset_after=True,\n",
        "            name=f\"gru_{i}\",\n",
        "        )\n",
        "        x = layers.Bidirectional(\n",
        "            recurrent, name=f\"bidirectional_{i}\", merge_mode=\"concat\"\n",
        "        )(x)\n",
        "        if i < rnn_layers:\n",
        "            x = layers.Dropout(rate=0.5)(x)\n",
        "    # Dense layer\n",
        "    x = layers.Dense(units=rnn_units * 2, name=\"dense_1\")(x)\n",
        "    x = layers.ReLU(name=\"dense_1_relu\")(x)\n",
        "    x = layers.Dropout(rate=0.5)(x)\n",
        "    # Classification layer\n",
        "    output = layers.Dense(units=output_dim + 1, activation=\"softmax\")(x)\n",
        "    # Model\n",
        "    model = keras.Model(input_spectrogram, output, name=\"DeepSpeech_2\")\n",
        "    # Optimizer\n",
        "    opt = keras.optimizers.Adam(learning_rate=1e-4)\n",
        "    # Compile the model and return\n",
        "    model.compile(optimizer=opt, loss=CTCLoss)\n",
        "    return model\n",
        "\n",
        "\n",
        "# Get the model\n",
        "model = build_model(\n",
        "    input_dim=fft_length // 2 + 1,\n",
        "    output_dim=char_to_num.vocabulary_size(),\n",
        "    rnn_units=10,\n",
        ")\n",
        "model.summary(line_length=110)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNh8FdmEqp5l",
        "outputId": "4b401408-65aa-4f17-b7e9-09fa54e4bca1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"DeepSpeech_2\"\n",
            "______________________________________________________________________________________________________________\n",
            " Layer (type)                                    Output Shape                                Param #          \n",
            "==============================================================================================================\n",
            " input (InputLayer)                              [(None, None, 193)]                         0                \n",
            "                                                                                                              \n",
            " expand_dim (Reshape)                            (None, None, 193, 1)                        0                \n",
            "                                                                                                              \n",
            " conv_1 (Conv2D)                                 (None, None, 97, 32)                        14432            \n",
            "                                                                                                              \n",
            " conv_1_bn (BatchNormalization)                  (None, None, 97, 32)                        128              \n",
            "                                                                                                              \n",
            " conv_1_relu (ReLU)                              (None, None, 97, 32)                        0                \n",
            "                                                                                                              \n",
            " conv_2 (Conv2D)                                 (None, None, 49, 32)                        236544           \n",
            "                                                                                                              \n",
            " conv_2_bn (BatchNormalization)                  (None, None, 49, 32)                        128              \n",
            "                                                                                                              \n",
            " conv_2_relu (ReLU)                              (None, None, 49, 32)                        0                \n",
            "                                                                                                              \n",
            " reshape (Reshape)                               (None, None, 1568)                          0                \n",
            "                                                                                                              \n",
            " bidirectional_1 (Bidirectional)                 (None, None, 20)                            94800            \n",
            "                                                                                                              \n",
            " dropout (Dropout)                               (None, None, 20)                            0                \n",
            "                                                                                                              \n",
            " bidirectional_2 (Bidirectional)                 (None, None, 20)                            1920             \n",
            "                                                                                                              \n",
            " dropout_1 (Dropout)                             (None, None, 20)                            0                \n",
            "                                                                                                              \n",
            " bidirectional_3 (Bidirectional)                 (None, None, 20)                            1920             \n",
            "                                                                                                              \n",
            " dropout_2 (Dropout)                             (None, None, 20)                            0                \n",
            "                                                                                                              \n",
            " bidirectional_4 (Bidirectional)                 (None, None, 20)                            1920             \n",
            "                                                                                                              \n",
            " dropout_3 (Dropout)                             (None, None, 20)                            0                \n",
            "                                                                                                              \n",
            " bidirectional_5 (Bidirectional)                 (None, None, 20)                            1920             \n",
            "                                                                                                              \n",
            " dense_1 (Dense)                                 (None, None, 20)                            420              \n",
            "                                                                                                              \n",
            " dense_1_relu (ReLU)                             (None, None, 20)                            0                \n",
            "                                                                                                              \n",
            " dropout_4 (Dropout)                             (None, None, 20)                            0                \n",
            "                                                                                                              \n",
            " dense (Dense)                                   (None, None, 223)                           4683             \n",
            "                                                                                                              \n",
            "==============================================================================================================\n",
            "Total params: 358,815\n",
            "Trainable params: 358,687\n",
            "Non-trainable params: 128\n",
            "______________________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A utility function to decode the output of the network\n",
        "def decode_batch_predictions(pred):\n",
        "    input_len = np.ones(pred.shape[0]) * pred.shape[1]\n",
        "    # Use greedy search. For complex tasks, you can use beam search\n",
        "    results = keras.backend.ctc_decode(pred, input_length=input_len, greedy=True)[0][0]\n",
        "    # Iterate over the results and get back the text\n",
        "    output_text = []\n",
        "    for result in results:\n",
        "        result = tf.strings.reduce_join(num_to_char(result)).numpy().decode(\"utf-8\")\n",
        "        output_text.append(result)\n",
        "    return output_text\n",
        "\n",
        "\n",
        "# A callback class to output a few transcriptions during training\n",
        "class CallbackEval(keras.callbacks.Callback):\n",
        "    \"\"\"Displays a batch of outputs after every epoch.\"\"\"\n",
        "\n",
        "    def __init__(self, dataset):\n",
        "        super().__init__()\n",
        "        self.dataset = dataset\n",
        "\n",
        "    def on_epoch_end(self, epoch: int, logs=None):\n",
        "        predictions = []\n",
        "        targets = []\n",
        "        for batch in self.dataset:\n",
        "            X, y = batch\n",
        "            batch_predictions = model.predict(X)\n",
        "            batch_predictions = decode_batch_predictions(batch_predictions)\n",
        "            predictions.extend(batch_predictions)\n",
        "            for label in y:\n",
        "                label = (\n",
        "                    tf.strings.reduce_join(num_to_char(label)).numpy().decode(\"utf-8\")\n",
        "                )\n",
        "                targets.append(label)\n",
        "        wer_score = wer(targets, predictions)\n",
        "        print(\"-\" * 100)\n",
        "        print(f\"Word Error Rate: {wer_score:.4f}\")\n",
        "        print(\"-\" * 100)\n",
        "        for i in np.random.randint(0, len(predictions), 2):\n",
        "            print(f\"Target    : {targets[i]}\")\n",
        "            print(f\"Prediction: {predictions[i]}\")\n",
        "            print(\"-\" * 100)"
      ],
      "metadata": {
        "id": "ioDNjjG2qqet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "ORXAx0jpb-Y2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's check results on more validation samples\n",
        "predictions = []\n",
        "targets = []\n",
        "for batch in validation_dataset:\n",
        "    X, y = batch\n",
        "    batch_predictions = model.predict(X)\n",
        "    batch_predictions = decode_batch_predictions(batch_predictions)\n",
        "    predictions.extend(batch_predictions)\n",
        "    for label in y:\n",
        "        label = tf.strings.reduce_join(num_to_char(label)).numpy().decode(\"utf-8\")\n",
        "        targets.append(label)\n",
        "wer_score = wer(targets, predictions)\n",
        "print(\"-\" * 100)\n",
        "print(f\"Word Error Rate: {wer_score:.4f}\")\n",
        "print(\"-\" * 100)\n",
        "for i in np.random.randint(0, len(predictions), 5):\n",
        "    print(f\"Target    : {targets[i]}\")\n",
        "    print(f\"Prediction: {predictions[i]}\")\n",
        "    print(\"-\" * 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHJ1qTbbUGna",
        "outputId": "97073dc8-63f8-40d8-d6fa-2d136bc2e03e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------------------------------------------\n",
            "Word Error Rate: 1.0000\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Target    : ጦርነቱ ከ ተጀመረ በኋላ ኢትዮጵያውያ ን በ ኤርትራ ሹመት እ ያገኙ ነው\n",
            "Prediction: ሀባሀጢፅጢዣጢዣጢዣጢፀጢፀጢዣጢፀዣጢዣጢፀዣጢዣጢሿባፒባሀፅጂነዪጂጷፆቭፆቭሁቭሁሀ\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Target    : ጦርነቱ ከ ተጀመረ በኋላ ኢትዮጵያውያ ን በ ኤርትራ ሹመት እ ያገኙ ነው\n",
            "Prediction: ሀባሀጢፅጢዣጢዣጢዣጢፀጢፀጢዣጢፀዣጢዣጢፀዣጢዣጢሿባፒባሀፅጂነዪጂጷፆቭፆቭሁቭሁሀ\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Target    : ኮንፍረንሱ ን የ መሩት ስር ዊሊያም ራሪ አዲስ ፎረ ም ለአፍሪካ ኢንቬስተመንት አዲስ ራእይ እንደሆነ ገልዋል\n",
            "Prediction: ሀጢዣፀጢፀዣጢሿቼፒጢፀሿጢዣጢዣፒጢዣጢዣጢፀጢዣጢዣጢዣፀጢዣጢዣጢዣጢዣጢዣጢዣጢሿባሀፅጭጢዪጂጷጂፆቭሁቭሁሀ \n",
            "----------------------------------------------------------------------------------------------------\n",
            "Target    : በ ተጐራባ ች አገሮች ኤርትራ ሶማሊያ እና ኢትዮጵያ የተከበበ ችው ጅቡቲ በጠረፎ ቿ መረጋጋት ን ለ ማየት ት ሻለች\n",
            "Prediction: ሀሿባጢፀዣፀጢዣጢፀጢፀዣፀጢዣጢፀዣጢዣጢዣፀጢዣጢዣጢፀዣግጢፀዣጢዣጢሿባፒባሀጢሿባዣግጢጂፆቭሁቭሁሀ\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Target    : የኤርትራ መንግስት ከ ደረሰበት የ ኢኮኖሚ ቀውስ ለ ማገገም የ ቦንድ ሽያጭ እንቅስቃሴ የ መገናኛ ብዙሀን አደባባይ እያ ወጡ ናቸው\n",
            "Prediction: ሀጢግዣጢዣጢዣጢሿዣጢሿጢሿባዣጢዣጢግጢዣጢሿባጢዣጢዣጢዣጢዣጢዣጢሿዣቼፒጢሿቼጢሿባሀጢግዪጂጷጂፆቭፆቭሁቭሁሀ\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the number of epochs.\n",
        "epochs = 10\n",
        "# Callback function to check transcription on the val set.\n",
        "validation_callback = CallbackEval(validation_dataset)\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=validation_dataset,\n",
        "    epochs=epochs,\n",
        "    callbacks=[validation_callback],\n",
        ")"
      ],
      "metadata": {
        "id": "Ek2ELMM65w3m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e6e8805-0077-46cc-e9e0-b2d6998dee95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "30/30 [==============================] - ETA: 0s - loss: 1507.1711----------------------------------------------------------------------------------------------------\n",
            "Word Error Rate: 1.0000\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Target    : በ ደህንነት መዋቅር ውስጥ ከ ገባ ን በ ተለያዩ ክፍሎች እንዳ ሻቸው ይ በ ተናሉ ከ ማህበ ሮቻችን ም በ ማስወጣት ወታደራዊ ህይወት እንድን ላበስ ልን ሆን እንችላ ለ ን የሚ ል ስጋት እንደ ፈጠረ ባቸው አስረድ ተዋል\n",
            "Prediction: \n",
            "----------------------------------------------------------------------------------------------------\n",
            "Target    : ይሁን ና ኢንቬስተ ሮች በ ኢንቬስትመን ቱ ውስጥ በ ሙሉ ሀይ ላቸው ለ መንቀሳቀስ እንዳል ቻሉ መንግስት የከተማ ቦታዎች ን በ ሞኖፖል በ መያዙ ችግር እንደ ተፈጠረ ባቸው ደጋግመው ገልዋል\n",
            "Prediction: \n",
            "----------------------------------------------------------------------------------------------------\n",
            "30/30 [==============================] - 234s 8s/step - loss: 1507.1711 - val_loss: 1668.6975\n",
            "Epoch 2/10\n",
            "30/30 [==============================] - ETA: 0s - loss: 1358.7823----------------------------------------------------------------------------------------------------\n",
            "Word Error Rate: 1.0000\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Target    : ይሁን ና ኢንቬስተ ሮች በ ኢንቬስትመን ቱ ውስጥ በ ሙሉ ሀይ ላቸው ለ መንቀሳቀስ እንዳል ቻሉ መንግስት የከተማ ቦታዎች ን በ ሞኖፖል በ መያዙ ችግር እንደ ተፈጠረ ባቸው ደጋግመው ገልዋል\n",
            "Prediction: \n",
            "----------------------------------------------------------------------------------------------------\n",
            "Target    : ከ የ አቅጣጫ ው እየ ደረሷቸው ያሉ መረጃዎች አሳሳቢ ችግሮች እየ ደረሱ መሆናቸው ን የሚ ጠቁሙ መሆናቸው ን ፕሬዝዳንቱ ተናግረ ዋል\n",
            "Prediction: \n",
            "----------------------------------------------------------------------------------------------------\n",
            "30/30 [==============================] - 203s 7s/step - loss: 1358.7823 - val_loss: 1470.4445\n",
            "Epoch 3/10\n",
            "30/30 [==============================] - ETA: 0s - loss: 1213.4910----------------------------------------------------------------------------------------------------\n",
            "Word Error Rate: 1.0000\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Target    : ጦርነቱ ከ ተጀመረ በኋላ ኢትዮጵያውያ ን በ ኤርትራ ሹመት እ ያገኙ ነው\n",
            "Prediction: \n",
            "----------------------------------------------------------------------------------------------------\n",
            "Target    : በ ድሬዳዋ ካውንስል የሚገኙ የመንግስት ቤቶች መካከል ሁለት ሺህ ያህሉ እንዲ ሸጡ ውሳኔ ማግኘታቸው ን ከተገኘ ው ዜና ለ መረዳት ተችሏል\n",
            "Prediction: \n",
            "----------------------------------------------------------------------------------------------------\n",
            "30/30 [==============================] - 201s 7s/step - loss: 1213.4910 - val_loss: 1278.1827\n",
            "Epoch 4/10\n",
            "30/30 [==============================] - ETA: 0s - loss: 1077.7937----------------------------------------------------------------------------------------------------\n",
            "Word Error Rate: 1.0000\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Target    : በ ድሬዳዋ ካውንስል የሚገኙ የመንግስት ቤቶች መካከል ሁለት ሺህ ያህሉ እንዲ ሸጡ ውሳኔ ማግኘታቸው ን ከተገኘ ው ዜና ለ መረዳት ተችሏል\n",
            "Prediction: \n",
            "----------------------------------------------------------------------------------------------------\n",
            "Target    : የኤርትራ መንግስት ከ ደረሰበት የ ኢኮኖሚ ቀውስ ለ ማገገም የ ቦንድ ሽያጭ እንቅስቃሴ የ መገናኛ ብዙሀን አደባባይ እያ ወጡ ናቸው\n",
            "Prediction: \n",
            "----------------------------------------------------------------------------------------------------\n",
            "30/30 [==============================] - 202s 7s/step - loss: 1077.7937 - val_loss: 1099.5721\n",
            "Epoch 5/10\n",
            "30/30 [==============================] - ETA: 0s - loss: 955.9703----------------------------------------------------------------------------------------------------\n",
            "Word Error Rate: 1.0000\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Target    : ከ የ አቅጣጫ ው እየ ደረሷቸው ያሉ መረጃዎች አሳሳቢ ችግሮች እየ ደረሱ መሆናቸው ን የሚ ጠቁሙ መሆናቸው ን ፕሬዝዳንቱ ተናግረ ዋል\n",
            "Prediction: \n",
            "----------------------------------------------------------------------------------------------------\n",
            "Target    : በ ደህንነት መዋቅር ውስጥ ከ ገባ ን በ ተለያዩ ክፍሎች እንዳ ሻቸው ይ በ ተናሉ ከ ማህበ ሮቻችን ም በ ማስወጣት ወታደራዊ ህይወት እንድን ላበስ ልን ሆን እንችላ ለ ን የሚ ል ስጋት እንደ ፈጠረ ባቸው አስረድ ተዋል\n",
            "Prediction: \n",
            "----------------------------------------------------------------------------------------------------\n",
            "30/30 [==============================] - 202s 7s/step - loss: 955.9703 - val_loss: 941.4310\n",
            "Epoch 6/10\n",
            "30/30 [==============================] - ETA: 0s - loss: 845.3013----------------------------------------------------------------------------------------------------\n",
            "Word Error Rate: 1.0000\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Target    : በ ደህንነት መዋቅር ውስጥ ከ ገባ ን በ ተለያዩ ክፍሎች እንዳ ሻቸው ይ በ ተናሉ ከ ማህበ ሮቻችን ም በ ማስወጣት ወታደራዊ ህይወት እንድን ላበስ ልን ሆን እንችላ ለ ን የሚ ል ስጋት እንደ ፈጠረ ባቸው አስረድ ተዋል\n",
            "Prediction: \n",
            "----------------------------------------------------------------------------------------------------\n",
            "Target    : ከ የ አቅጣጫ ው እየ ደረሷቸው ያሉ መረጃዎች አሳሳቢ ችግሮች እየ ደረሱ መሆናቸው ን የሚ ጠቁሙ መሆናቸው ን ፕሬዝዳንቱ ተናግረ ዋል\n",
            "Prediction: \n",
            "----------------------------------------------------------------------------------------------------\n",
            "30/30 [==============================] - 199s 7s/step - loss: 845.3013 - val_loss: 808.5606\n",
            "Epoch 7/10\n",
            "30/30 [==============================] - ETA: 0s - loss: 753.7410----------------------------------------------------------------------------------------------------\n",
            "Word Error Rate: 1.0000\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Target    : ከ የ አቅጣጫ ው እየ ደረሷቸው ያሉ መረጃዎች አሳሳቢ ችግሮች እየ ደረሱ መሆናቸው ን የሚ ጠቁሙ መሆናቸው ን ፕሬዝዳንቱ ተናግረ ዋል\n",
            "Prediction: \n",
            "----------------------------------------------------------------------------------------------------\n",
            "Target    : ከውጭ አዳዲስ ቴክኖሎጂ ዎችን ለ ማስገባት የሚ ያደርጋቸው ሙከራ ዎች በ ቢሮክራሲ ው መንዛ ዛት ከንቱ ሆነው ቀርተው በታል\n",
            "Prediction: \n",
            "----------------------------------------------------------------------------------------------------\n",
            "30/30 [==============================] - 197s 6s/step - loss: 753.7410 - val_loss: 701.9061\n",
            "Epoch 8/10\n",
            "30/30 [==============================] - ETA: 0s - loss: 675.0587----------------------------------------------------------------------------------------------------\n",
            "Word Error Rate: 1.0000\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Target    : መገንጠል ን መርጦ ድም ከተሰጠ በኋላ የ ኢትዮጵያ ን ፓስፖርት መያዝ ብቻ ውን ኢትዮጵያዊ ዜግነት ን እንደማያ ሰጥ ተገለጠ\n",
            "Prediction: \n",
            "----------------------------------------------------------------------------------------------------\n",
            "Target    : መገንጠል ን መርጦ ድም ከተሰጠ በኋላ የ ኢትዮጵያ ን ፓስፖርት መያዝ ብቻ ውን ኢትዮጵያዊ ዜግነት ን እንደማያ ሰጥ ተገለጠ\n",
            "Prediction: \n",
            "----------------------------------------------------------------------------------------------------\n",
            "30/30 [==============================] - 198s 7s/step - loss: 675.0587 - val_loss: 619.9724\n",
            "Epoch 9/10\n",
            "30/30 [==============================] - ETA: 0s - loss: 614.6801----------------------------------------------------------------------------------------------------\n",
            "Word Error Rate: 1.0000\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Target    : በ ተጐራባ ች አገሮች ኤርትራ ሶማሊያ እና ኢትዮጵያ የተከበበ ችው ጅቡቲ በጠረፎ ቿ መረጋጋት ን ለ ማየት ት ሻለች\n",
            "Prediction: \n",
            "----------------------------------------------------------------------------------------------------\n",
            "Target    : በ ተጐራባ ች አገሮች ኤርትራ ሶማሊያ እና ኢትዮጵያ የተከበበ ችው ጅቡቲ በጠረፎ ቿ መረጋጋት ን ለ ማየት ት ሻለች\n",
            "Prediction: \n",
            "----------------------------------------------------------------------------------------------------\n",
            "30/30 [==============================] - 198s 7s/step - loss: 614.6801 - val_loss: 559.0497\n",
            "Epoch 10/10\n",
            "30/30 [==============================] - ETA: 0s - loss: 564.7430----------------------------------------------------------------------------------------------------\n",
            "Word Error Rate: 1.0000\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Target    : መገንጠል ን መርጦ ድም ከተሰጠ በኋላ የ ኢትዮጵያ ን ፓስፖርት መያዝ ብቻ ውን ኢትዮጵያዊ ዜግነት ን እንደማያ ሰጥ ተገለጠ\n",
            "Prediction: \n",
            "----------------------------------------------------------------------------------------------------\n",
            "Target    : በ ተጐራባ ች አገሮች ኤርትራ ሶማሊያ እና ኢትዮጵያ የተከበበ ችው ጅቡቲ በጠረፎ ቿ መረጋጋት ን ለ ማየት ት ሻለች\n",
            "Prediction: \n",
            "----------------------------------------------------------------------------------------------------\n",
            "30/30 [==============================] - 196s 6s/step - loss: 564.7430 - val_loss: 515.2017\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Space Exploration"
      ],
      "metadata": {
        "id": "bwSs40fQ6HFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the model\n",
        "model = build_model(\n",
        "    input_dim=fft_length // 2 + 1,\n",
        "    output_dim=char_to_num.vocabulary_size(),\n",
        "    rnn_layers = 6,\n",
        "    rnn_units=10,\n",
        ")\n",
        "model.summary(line_length=110)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "b16aeBog3a_U",
        "outputId": "0a1a3525-89b0-4c51-fe47-3e7286ebe6c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-30f4effdc714>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0moutput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchar_to_num\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mrnn_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mrnn_units\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m110\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-b051d7840805>\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(input_dim, output_dim, rnn_layers, rnn_units)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_dim\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"softmax\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;31m# Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_spectrogram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"DeepSpeech_2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0;31m# Optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    627\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, inputs, outputs, name, trainable, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m                   for t in tf.nest.flatten(inputs)]):\n\u001b[1;32m    145\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunctional_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone_graph_nodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_automatic_dependency_tracking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    627\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36m_init_graph_network\u001b[0;34m(self, inputs, outputs)\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;31m# Keep track of the network's nodes and layers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     nodes, nodes_by_depth, layers, _ = _map_graph_network(\n\u001b[0;32m--> 230\u001b[0;31m         self.inputs, self.outputs)\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_network_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nodes_by_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnodes_by_depth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36m_map_graph_network\u001b[0;34m(inputs, outputs)\u001b[0m\n\u001b[1;32m   1048\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mall_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m       raise ValueError(\n\u001b[0;32m-> 1050\u001b[0;31m           \u001b[0;34mf'The name \"{name}\" is used {all_names.count(name)} '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m           'times in the model. All layer names should be unique.')\n\u001b[1;32m   1052\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mnetwork_nodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnodes_by_depth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers_by_depth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The name \"dense_1\" is used 2 times in the model. All layer names should be unique."
          ]
        }
      ]
    }
  ]
}